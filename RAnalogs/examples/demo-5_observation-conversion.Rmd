---
title: "Demo 5: How to format observations for AnEn"
author: "Weiming Hu"
date: "November 18, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
stopifnot(packageVersion('RAnEn')>="3.7.0")

library(sp)
library(dplyr)
library(tidyr)
library(RAnEn)
library(ggplot2)
```

## Introduction

Tutorial shows an example of converting a data frame to a list with the format that is required for analog generation.

The accepted format for observations can be found [here](https://weiming-hu.github.io/AnalogsEnsemble/2019/01/16/NetCDF-File-Types.html#observations). It can be either an NetCDF file on the disk or a list in R. In this tutorial, we are going to create the accepted observation list in R. This R list can then be written to your disk as an NetCDF file, read back into R, and used by `RAnEn` easily.

## Data Access

The example dataset we are going to use is the [air quality data from EPA](https://aqs.epa.gov/aqsweb/airdata/download_files.html#Raw). Let's download a data file first.

```{r}
# Define the observation file to read
obs.file <- 'hourly_44201_2019.csv'

if (!file.exists(obs.file)) {
  obs.file.zip <- 'hourly_44201_2019.zip'
  
  # Download the file if it does not exist
  url <- paste0('https://aqs.epa.gov/aqsweb/airdata/', obs.file.zip)
  stopifnot(download.file(url = url, destfile = obs.file.zip) == 0)
  
  # Decompress the zip file to the current folder
  unzip(obs.file.zip, exdir = '.')
  
  # Remove intermediate files
  file.remove(obs.file.zip)
} else {
  cat('File exists. Skip downloading!\n')
}
```

After the download, we can read the data file into R.

```{r}
# If you use a text editor to examine the CSV file, you will
# understand the following parameters:
# 
# - sep: Columns are seperated by comma;
# - quote: Cells are quoted with double quotes;
# - header: The first row contains header information, rather than data entires.
# - stringsAsFactors: Do not convert strings to factors for the ease of processing
# 
obs <- read.table(obs.file, sep = ',', quote = '"', header = T, stringsAsFactors = F)

# Let's only process 1% of the total amount of data.
# Sample data randomly.
# 
obs <- obs[sample(nrow(obs), floor(nrow(obs) * 0.01)), ]

# Time columns are characters. Let's create POSIXct times.
obs$POSIX <- as.POSIXct(
  paste(obs$Date.GMT, obs$Time.GMT),
  format = '%Y-%m-%d %H:%M', tz = 'UTC')
```

## Data Preprocessing

Let's take a look at what the file includes.

```{r}
str(obs)
```

Always remember that we will convert this data frame into a list that contains information for station coordinates, times, parameters, and data. The data will be a 3-dimensional array.

Let's examine how many parameters there are in the file.

```{r}
unique(obs$Parameter.Name)
```

Nice. There is only one parameter, which is Ozone. So presumably, our first dimension of the observation data array should be 1.

Next, let's take a look at how many unique stations there are in the file. This is slightly more complicated than parameters. We cannot just look at the unique values of x coordinates or y coordinates individually. We need to look at x and y coordinates at the same time to determine whether locations are duplicated or unique.

```{r}
# HOLD ON!
# 
# Notice that there are two datums used in the dataset.
# 
# While you can definitely argue that the differences between these
# two datums for North America is not discernible, I include the 
# code for transformation just as a reference for other situations.
# 
unique(obs$Datum)

# Let's reproject NAD83 to WGS84.
i.row.NAD83 <- which(obs$Datum == 'NAD83')
pts.NAD83 <- SpatialPoints(
  obs[i.row.NAD83, c('Longitude', 'Latitude')],
  proj4string = CRS('+proj=longlat +datum=NAD83'))
pts.WGS84 <- spTransform(
  pts.NAD83, CRS('+proj=longlat +datum=WGS84'))

# Actually, coordinates do not change.
all.equal(coordinates(pts.NAD83), coordinates(pts.WGS84))

# Change the coordinates and datums in observations
obs$Datum[i.row.NAD83] <- 'WGS84'
obs[i.row.NAD83, c('Longitude', 'Latitude')] <- coordinates(pts.WGS84)

# Let's create a unique identifier for each unique point
obs$Station.ID <- obs %>% group_indices(Latitude, Longitude)

# Now, let's see how many unique identifiers are for stations
length(unique(obs$Station.ID))
```

## Create the Observation List

Now that we have examined the dataset, we can go ahead and create the observation list. Keep in mind that, our list should have [these members as specified in the convention](https://weiming-hu.github.io/AnalogsEnsemble/2019/01/16/NetCDF-File-Types.html#observations).

```{r}
# We leave out some optional parameters because
# we do not have them in this example.
# 
observations <- list(
  ParameterNames = NA, StationNames = NA,
  Xs = NA, Ys = NA, Times = NA, Data = NA)

# Extract the unique coordinates
unique.pts <- obs %>%
  distinct(Station.ID, .keep_all = T) %>%
  select(Longitude, Latitude, Station.ID)

# Assign values to the observation list
observations$ParameterNames <- 'Ozone'
observations$StationNames <- unique.pts$Station.ID
observations$Xs <- unique.pts$Longitude
observations$Ys <- unique.pts$Latitude
```

So far so good! We have tackled most of the tasks by far. The remaining question is how to assign times and data values.

There is a consideration that times might not be formatted in the dataset. In other words, times might not have the same intervals, and there might be missing values at certain times. So we might need to interpolate values to 1) fill up the missing values and 2) to have a formatted time seires with equal-length intervals.

For the sake of simplicity, let's take a look at one station for only one parameter.

```{r}
station.id <- 4
parameter <- 'Ozone'

# Subset the observations to the selected station and the selected parameter
obs.subset <- obs %>%
  subset(Station.ID == station.id & Parameter.Name == parameter) %>%
  select(POSIX, Sample.Measurement)

# Let's see how different the available times are for stations and paramters.
# You can change the station ID to try this out. Presumably, you will see
# different numbers of availble times for each station and parameter.
# 
# Of course, in this specific situation, we only have one paramter. So
# you can only paly with station ID for now.
# 
dim(obs.subset)
range(obs.subset$POSIX)
head(obs.subset)
```

To create a time series with equal-length intervals, we define the following times.

```{r}
start.time <- min(obs$POSIX)
end.time <- max(obs$POSIX)
observations$Times <- seq(
  from = start.time, to = end.time, by = 'hour')
```

And we are going to assign values for each of the time slot if the values can be found. Notice that, in this example, we do not interpolate measurements because measurements are already hourly data.

```{r}
# Allocate the array for observation data
observations$Data <- array(NA, dim = c(length(observations$ParameterNames),
                                       length(observations$Xs),
                                       length(observations$Times)))

i.par <- which(observations$ParameterNames == 'Ozone')

for (station.id in observations$StationNames) {
  
  # Which position to write the data
  i.station <- which(observations$StationNames == station.id)
  
  obs.subset <- obs %>%
    
    # Subset the observations to the selected station ID
    subset(Station.ID == station.id) %>%
    select(POSIX, Sample.Measurement) %>%
    
    # Append requested time slots
    rbind(data.frame(
    POSIX = observations$Times,
    Sample.Measurement = NA))
  
  # Remove duplicated rows based on time
  obs.subset <- obs.subset[!duplicated(obs.subset$POSIX), ]
  
  # Sort rows based on time
  observations$Data[i.par, i.station, ] <- 
    obs.subset$Sample.Measurement[order(obs.subset$POSIX)]
}
```

## Finishing Up

At this point, we have finished creating the observation list in R. You can take a look at what we have created.

```{r}
class(observations)
names(observations)
length(observations$StationNames)
dim(observations$Data)
```

Or you can write this R list as an NetCDF file.

```{r}
writeNetCDF('Observations', observations, 'observations.nc')

# You can also read the NetCDF file
# 
# obs <- readObservations('observations.nc')
```

The final step, however, is to check whether data have been properly converted. Are there any bugs or unexpected situations happening during our conversion? It is a good practice to make sure there are no surprises in the data. Usually, this is done by manually comparing the converted and the original data. Here, I provide some examples.

```{r}
# Compare the locations
df <- rbind(
  data.frame(unique.pts[, c('Longitude', 'Latitude')],
             Dataset = 'Original'),
  data.frame(Longitude = observations$Xs,
             Latitude = observations$Ys,
             Dataset = 'Converted'))

ggplot(data = df) +
  geom_point(mapping = aes(x = Longitude, y = Latitude)) +
  coord_quickmap() +
  facet_wrap('Dataset') +
  theme_minimal()

# Compare measurements at a certain station
station.id <- 5

station.index <- which(station.id == observations$StationNames)
obs.subset <- obs %>%
  subset(Station.ID == station.id & Parameter.Name == 'Ozone') %>%
  select(POSIX, Sample.Measurement)

df <- rbind(
  data.frame(obs.subset, Dataset = 'Original'),
  data.frame(POSIX = observations$Times,
             Sample.Measurement = observations$Data[1, station.index, ],
             Dataset = 'Converted'))

ggplot(data = df, mapping = aes(
    x = POSIX, y = Sample.Measurement)) +
  geom_point() +
  facet_wrap('Dataset') +
  theme_minimal()

# Compare the summary statistics and histograms of measurements
par(mfrow = c(1, 2))
hist(obs$Sample.Measurement, main = 'Original')
hist(observations$Data, main = 'Converted')

print(summary(na.omit(obs$Sample.Measurement)))
print(summary(na.omit(as.numeric(observations$Data))))
```
